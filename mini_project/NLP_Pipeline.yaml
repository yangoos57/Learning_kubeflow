apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: nlp-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-01-21T16:34:43.742574',
    pipelines.kubeflow.org/pipeline_spec: '{"name": "NLP_Pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: nlp-pipeline
  templates:
  - name: create-inference-model
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'kserve==0.8.0' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install
        --quiet --no-warn-script-location 'kserve==0.8.0' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def create_inference_model():

            from kserve import (
                constants,
                KServeClient,
                V1beta1InferenceService,
                V1beta1InferenceServiceSpec,
                V1beta1PredictorSpec,
                V1beta1ModelSpec,
                V1beta1ModelFormat,
                V1beta1TorchServeSpec,
                utils,
            )
            from kubernetes import client

            client.V1ResourceRequirements(requests={"cpu": 1, "memory": "1G"})

            service_name = "pytorchserve2"
            namespace = "kubeflow-user-example-com"
            api_version = constants.KSERVE_GROUP + "/" + constants.KSERVE_V1BETA1_VERSION
            storage = "pvc://lee/torch_model"

            torchsvc = V1beta1InferenceService(
                api_version=api_version,
                kind=constants.KSERVE_KIND,
                metadata=client.V1ObjectMeta(
                    name=service_name, namespace=namespace, annotations={"sidecar.istio.io/inject": "false"}
                ),
                spec=V1beta1InferenceServiceSpec(
                    predictor=V1beta1PredictorSpec(
                        pytorch=(
                            V1beta1TorchServeSpec(
                                protocol_version="v1",
                                resources=client.V1ResourceRequirements(
                                    requests={"cpu": 1, "memory": "1G"}
                                ),
                                storage_uri=storage,
                            )
                        )
                    )
                ),
            )
            KServe = KServeClient()
            KServe.create(torchsvc)

        import argparse
        _parser = argparse.ArgumentParser(prog='Create inference model', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = create_inference_model(**_parsed_args)
      image: python:3.7
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''kserve==0.8.0'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''kserve==0.8.0''
          --user) && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf
          \"%s\" \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n",
          "def create_inference_model():\n\n    from kserve import (\n        constants,\n        KServeClient,\n        V1beta1InferenceService,\n        V1beta1InferenceServiceSpec,\n        V1beta1PredictorSpec,\n        V1beta1ModelSpec,\n        V1beta1ModelFormat,\n        V1beta1TorchServeSpec,\n        utils,\n    )\n    from
          kubernetes import client\n\n    client.V1ResourceRequirements(requests={\"cpu\":
          1, \"memory\": \"1G\"})\n\n    service_name = \"pytorchserve2\"\n    namespace
          = \"kubeflow-user-example-com\"\n    api_version = constants.KSERVE_GROUP
          + \"/\" + constants.KSERVE_V1BETA1_VERSION\n    storage = \"pvc://lee/torch_model\"\n\n    torchsvc
          = V1beta1InferenceService(\n        api_version=api_version,\n        kind=constants.KSERVE_KIND,\n        metadata=client.V1ObjectMeta(\n            name=service_name,
          namespace=namespace, annotations={\"sidecar.istio.io/inject\": \"false\"}\n        ),\n        spec=V1beta1InferenceServiceSpec(\n            predictor=V1beta1PredictorSpec(\n                pytorch=(\n                    V1beta1TorchServeSpec(\n                        protocol_version=\"v1\",\n                        resources=client.V1ResourceRequirements(\n                            requests={\"cpu\":
          1, \"memory\": \"1G\"}\n                        ),\n                        storage_uri=storage,\n                    )\n                )\n            )\n        ),\n    )\n    KServe
          = KServeClient()\n    KServe.create(torchsvc)\n\nimport argparse\n_parser
          = argparse.ArgumentParser(prog=''Create inference model'', description='''')\n_parsed_args
          = vars(_parser.parse_args())\n\n_outputs = create_inference_model(**_parsed_args)\n"],
          "image": "python:3.7"}}, "name": "Create inference model"}', pipelines.kubeflow.org/component_ref: '{}'}
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: lee}
  - name: creating-marfile
    container:
      args: [-c, 'cd pvc/torch_model; pip install torchserve torch-model-archiver
          torch-workflow-archiver; torch-model-archiver --model-name torch-model --version
          1.0 --serialized-file pytorch_model.bin --handler handler.py --extra-files
          config.json,vocab.txt --force; mkdir model-store; mv -f torch-model.mar
          model-store']
      command: [/bin/sh]
      image: python:3.9
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Creating Marfile, pipelines.kubeflow.org/max_cache_staleness: P0D}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: lee}
  - name: load-data
    container:
      args: []
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'pandas' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def load_data(
            # : OutputPath("csv"),
            # evaluation_path: OutputPath("csv"),
        ):

            import pandas as pd
            import os

            print("list_dir : \n ", os.listdir())

            # load data from github
            df_train = pd.read_csv(
                "https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/train.csv"
            )
            df_evaluation = pd.read_csv(
                "https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/validation.csv"
            )

            df_train.to_csv("pvc/train.csv", index=False)
            df_evaluation.to_csv("pvc/evaluation.csv", index=False)

            print("complete Loading Data")

        import argparse
        _parser = argparse.ArgumentParser(prog='Load data', description='')
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_data(**_parsed_args)
      image: python:3.7
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1 python3
          -m pip install --quiet --no-warn-script-location ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def load_data(\n    #
          : OutputPath(\"csv\"),\n    # evaluation_path: OutputPath(\"csv\"),\n):\n\n    import
          pandas as pd\n    import os\n\n    print(\"list_dir : \\n \", os.listdir())\n\n    #
          load data from github\n    df_train = pd.read_csv(\n        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/train.csv\"\n    )\n    df_evaluation
          = pd.read_csv(\n        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/validation.csv\"\n    )\n\n    df_train.to_csv(\"pvc/train.csv\",
          index=False)\n    df_evaluation.to_csv(\"pvc/evaluation.csv\", index=False)\n\n    print(\"complete
          Loading Data\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Load
          data'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_data(**_parsed_args)\n"], "image": "python:3.7"}}, "name": "Load
          data"}', pipelines.kubeflow.org/component_ref: '{}'}
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: lee}
  - name: nlp-pipeline
    dag:
      tasks:
      - name: create-inference-model
        template: create-inference-model
        dependencies: [creating-marfile]
      - name: creating-marfile
        template: creating-marfile
        dependencies: [train-model]
      - {name: load-data, template: load-data}
      - name: train-model
        template: train-model
        dependencies: [load-data]
  - name: train-model
    container:
      args: []
      command:
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - "def train_model(\n    # train_path:InputPath(\"csv\"),\n    # evaluation_path:\
        \ InputPath(\"csv\"),\n):\n\n    from transformers import (\n        DistilBertForSequenceClassification,\n\
        \        DistilBertTokenizer,\n        Trainer,\n        TrainingArguments,\n\
        \        TrainerCallback,\n    )\n\n    from datasets import Dataset\n\n \
        \   import os\n\n    print(\"list_dir : \\n \", os.listdir())\n    print(\"\
        list_dir : \\n \", os.getcwd())\n    os.chdir(\"/\")\n\n    # loading data\n\
        \    # train_dataset = Dataset.from_csv(train_path).select(range(100))\n \
        \   # evaluation_dataset = Dataset.from_csv(evaluation_path)\n\n    train_dataset\
        \ = Dataset.from_csv(\"pvc/train.csv\").select(range(32))\n    evaluation_dataset\
        \ = Dataset.from_csv(\"pvc/evaluation.csv\")\n\n    # tokenizing\n    tokenizer\
        \ = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n \
        \   def tokenize_function(item):\n        return tokenizer(item[\"text\"],\
        \ padding=\"max_length\", max_length=128, truncation=True)\n\n    train =\
        \ train_dataset.map(tokenize_function)\n    evaluation = evaluation_dataset.map(tokenize_function)\n\
        \n    print(\"complete Tokenizing\")\n\n    model = DistilBertForSequenceClassification.from_pretrained(\n\
        \        \"distilbert-base-uncased\", num_labels=len(set(train_dataset[\"\
        label\"]))\n    )\n\n    # BetterTransformer\n\n    tra_arg = TrainingArguments(\n\
        \        per_device_train_batch_size=8,\n        output_dir=\"test\",\n  \
        \      num_train_epochs=1,\n        logging_steps=2,\n        # evaluation_strategy=\"\
        epoch\",\n        disable_tqdm=True,\n        save_strategy=\"no\",\n    )\n\
        \n    class myCallback(TrainerCallback):\n        def on_log(self, args, state,\
        \ control, logs=None, **kwargs):\n            print(f\"{state.global_step}\
        \ Steps \")\n\n    trainer = Trainer(\n        model=model,\n        args=tra_arg,\n\
        \        train_dataset=train,\n        eval_dataset=evaluation,\n        callbacks=[myCallback],\n\
        \    )\n\n    trainer.train()\n\n    # Saving Tokenizer, Model\n    trainer.save_model(\"\
        pvc/torch_model\")\n    tokenizer.save_pretrained(\"pvc/torch_model\")\n\n\
        \    print(\"Saving Model & Tokenizer Complete !!\")\n\n    # config for torch\n\
        \    config = dict(\n        inference_address=\"http://0.0.0.0:8085\",\n\
        \        management_address=\"http://0.0.0.0:8085\",\n        metrics_address=\"\
        http://0.0.0.0:8082\",\n        grpc_inference_port=7070,\n        grpc_management_port=7071,\n\
        \        enable_envvars_config=\"true\",\n        install_py_dep_per_model=\"\
        true\",\n        model_store=\"model-store\",\n        model_snapshot={\n\
        \            \"name\": \"startup.cfg\",\n            \"modelCount\": 1,\n\
        \            \"models\": {\n                \"torch-model\": {  # Model Name\n\
        \                    \"1.0\": {\n                        \"defaultVersion\"\
        : \"true\",\n                        \"marName\": \"torch-model.mar\",\n \
        \                       \"minWorkers\": 1,\n                        \"maxWorkers\"\
        : 5,\n                        \"batchSize\": 1,\n                        \"\
        maxBatchDelay\": 500,\n                        \"responseTimeout\": 120,\n\
        \                    }\n                }\n            },\n        },\n  \
        \  )\n\n    # making config & config folder\n    if not os.path.exists('pvc/torch_model/config')\
        \ : \n        os.mkdir('pvc/torch_model/config')\n\n    with open(\"pvc/torch_model/config/config.properties\"\
        , \"w\") as f:\n        for i, j in config.items():\n            f.write(f\"\
        {i}={j}\\n\")\n        f.close()\n\n    print(\"Saving config.properties !!\"\
        )\n\n    # handler for torch\n    x = '''\nfrom abc import ABC\nimport json\n\
        import logging\nimport os\n\nimport torch\nfrom transformers import AutoModelForSequenceClassification,\
        \ AutoTokenizer\nfrom ts.torch_handler.base_handler import BaseHandler\n\n\
        logger = logging.getLogger(__name__)\n\nclass TransformersClassifierHandler(BaseHandler,\
        \ ABC):\n    def __init__(self):\n        super(TransformersClassifierHandler,\
        \ self).__init__()\n        self.initialized = False\n\n    def initialize(self,\
        \ ctx):\n        self.manifest = ctx.manifest\n\n        properties = ctx.system_properties\n\
        \        model_dir = properties.get(\"model_dir\")\n        self.device =\
        \ torch.device(\n            \"cuda:\" + str(properties.get(\"gpu_id\")) if\
        \ torch.cuda.is_available() else \"cpu\"\n        )\n\n        # Read model\
        \ serialize/pt file\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n\
        \        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n\n   \
        \     self.model.to(self.device)\n        # BetterTransformer\n        self.model.eval()\n\
        \n        logger.debug(f\"Transformer model from path {model_dir} loaded successfully\"\
        )\n\n        # Read the mapping file, index to object name\n        mapping_file_path\
        \ = os.path.join(model_dir, \"index_to_name.json\")\n\n        if os.path.isfile(mapping_file_path):\n\
        \            with open(mapping_file_path) as f:\n                self.mapping\
        \ = json.load(f)\n        else:\n            logger.warning(\n           \
        \     \"Missing the index_to_name.json file. Inference output will not include\
        \ class name.\"\n            )\n\n        self.initialized = True\n\n    def\
        \ preprocess(self, data):\n        \"\"\"Very basic preprocessing code - only\
        \ tokenizes.\n        Extend with your own preprocessing steps as needed.\n\
        \        \"\"\"\n        print(\"------- input data --------\")\n        print(data)\n\
        \        text = data[0].get(\"data\")\n        if text is None:\n        \
        \    text = data[0].get(\"body\")\n\n        logger.info(f\"Received text:\
        \ {text}\")\n\n        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True,\
        \ return_tensors=\"pt\")\n        return inputs\n\n    def inference(self,\
        \ inputs):\n        \"\"\"\n        Predict the class of a text using a trained\
        \ transformer model.\n        \"\"\"\n        # NOTE: This makes the assumption\
        \ that your model expects text to be tokenized\n        # with \"input_ids\"\
        \ and \"token_type_ids\" - which is true for some popular transformer models,\
        \ e.g. bert.\n        # If your transformer model expects different tokenization,\
        \ adapt this code to suit\n        # its expected input format.\n        inputs\
        \ = inputs.to(self.device)\n\n        prediction = self.model(**inputs)[0].argmax().item()\n\
        \        logger.info(f\"Model predicted: {prediction}\")\n\n        if self.mapping:\n\
        \            prediction = self.mapping[str(prediction)]\n        return [prediction]\n\
        \n    def postprocess(self, inference_output):\n        # TODO: Add any needed\
        \ post-processing of the model predictions here\n        logger.info(f\"Model\
        \ Name: {self.model.config._name_or_path}\")\n        logger.info(f\"Model\
        \ predicted: {inference_output}\")\n        return inference_output\n\n_service\
        \ = TransformersClassifierHandler()\n\ndef handle(data, context):\n    try:\n\
        \        if not _service.initialized:\n            _service.initialize(context)\n\
        \n        if data is None:\n            return None\n\n        data = _service.preprocess(data)\n\
        \        data = _service.inference(data)\n        data = _service.postprocess(data)\n\
        \n        return data\n    except Exception as e:\n        raise e\n\n   \
        \ '''\n    with open(\"pvc/torch_model/handler.py\", \"w\") as f:\n      \
        \  f.write(x)\n    f.close()\n\n    print(\"Saving handler.py complete !!\"\
        )\n\nimport argparse\n_parser = argparse.ArgumentParser(prog='Train model',\
        \ description='')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs =\
        \ train_model(**_parsed_args)\n"
      image: 679oose/basepython:1.0
      resources:
        limits: {cpu: '2', memory: 4G}
      volumeMounts:
      - {mountPath: pvc, name: test-lee}
    metadata:
      annotations: {pipelines.kubeflow.org/task_display_name: Finetuning Text Classification
          Model, pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": [], "command": ["sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\"
          \"$0\" > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def
          train_model(\n    # train_path:InputPath(\"csv\"),\n    # evaluation_path:
          InputPath(\"csv\"),\n):\n\n    from transformers import (\n        DistilBertForSequenceClassification,\n        DistilBertTokenizer,\n        Trainer,\n        TrainingArguments,\n        TrainerCallback,\n    )\n\n    from
          datasets import Dataset\n\n    import os\n\n    print(\"list_dir : \\n \",
          os.listdir())\n    print(\"list_dir : \\n \", os.getcwd())\n    os.chdir(\"/\")\n\n    #
          loading data\n    # train_dataset = Dataset.from_csv(train_path).select(range(100))\n    #
          evaluation_dataset = Dataset.from_csv(evaluation_path)\n\n    train_dataset
          = Dataset.from_csv(\"pvc/train.csv\").select(range(32))\n    evaluation_dataset
          = Dataset.from_csv(\"pvc/evaluation.csv\")\n\n    # tokenizing\n    tokenizer
          = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n\n    def
          tokenize_function(item):\n        return tokenizer(item[\"text\"], padding=\"max_length\",
          max_length=128, truncation=True)\n\n    train = train_dataset.map(tokenize_function)\n    evaluation
          = evaluation_dataset.map(tokenize_function)\n\n    print(\"complete Tokenizing\")\n\n    model
          = DistilBertForSequenceClassification.from_pretrained(\n        \"distilbert-base-uncased\",
          num_labels=len(set(train_dataset[\"label\"]))\n    )\n\n    # BetterTransformer\n\n    tra_arg
          = TrainingArguments(\n        per_device_train_batch_size=8,\n        output_dir=\"test\",\n        num_train_epochs=1,\n        logging_steps=2,\n        #
          evaluation_strategy=\"epoch\",\n        disable_tqdm=True,\n        save_strategy=\"no\",\n    )\n\n    class
          myCallback(TrainerCallback):\n        def on_log(self, args, state, control,
          logs=None, **kwargs):\n            print(f\"{state.global_step} Steps \")\n\n    trainer
          = Trainer(\n        model=model,\n        args=tra_arg,\n        train_dataset=train,\n        eval_dataset=evaluation,\n        callbacks=[myCallback],\n    )\n\n    trainer.train()\n\n    #
          Saving Tokenizer, Model\n    trainer.save_model(\"pvc/torch_model\")\n    tokenizer.save_pretrained(\"pvc/torch_model\")\n\n    print(\"Saving
          Model & Tokenizer Complete !!\")\n\n    # config for torch\n    config =
          dict(\n        inference_address=\"http://0.0.0.0:8085\",\n        management_address=\"http://0.0.0.0:8085\",\n        metrics_address=\"http://0.0.0.0:8082\",\n        grpc_inference_port=7070,\n        grpc_management_port=7071,\n        enable_envvars_config=\"true\",\n        install_py_dep_per_model=\"true\",\n        model_store=\"model-store\",\n        model_snapshot={\n            \"name\":
          \"startup.cfg\",\n            \"modelCount\": 1,\n            \"models\":
          {\n                \"torch-model\": {  # Model Name\n                    \"1.0\":
          {\n                        \"defaultVersion\": \"true\",\n                        \"marName\":
          \"torch-model.mar\",\n                        \"minWorkers\": 1,\n                        \"maxWorkers\":
          5,\n                        \"batchSize\": 1,\n                        \"maxBatchDelay\":
          500,\n                        \"responseTimeout\": 120,\n                    }\n                }\n            },\n        },\n    )\n\n    #
          making config & config folder\n    if not os.path.exists(''pvc/torch_model/config'')
          : \n        os.mkdir(''pvc/torch_model/config'')\n\n    with open(\"pvc/torch_model/config/config.properties\",
          \"w\") as f:\n        for i, j in config.items():\n            f.write(f\"{i}={j}\\n\")\n        f.close()\n\n    print(\"Saving
          config.properties !!\")\n\n    # handler for torch\n    x = ''''''\nfrom
          abc import ABC\nimport json\nimport logging\nimport os\n\nimport torch\nfrom
          transformers import AutoModelForSequenceClassification, AutoTokenizer\nfrom
          ts.torch_handler.base_handler import BaseHandler\n\nlogger = logging.getLogger(__name__)\n\nclass
          TransformersClassifierHandler(BaseHandler, ABC):\n    def __init__(self):\n        super(TransformersClassifierHandler,
          self).__init__()\n        self.initialized = False\n\n    def initialize(self,
          ctx):\n        self.manifest = ctx.manifest\n\n        properties = ctx.system_properties\n        model_dir
          = properties.get(\"model_dir\")\n        self.device = torch.device(\n            \"cuda:\"
          + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\"\n        )\n\n        #
          Read model serialize/pt file\n        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n        self.tokenizer
          = AutoTokenizer.from_pretrained(model_dir)\n\n        self.model.to(self.device)\n        #
          BetterTransformer\n        self.model.eval()\n\n        logger.debug(f\"Transformer
          model from path {model_dir} loaded successfully\")\n\n        # Read the
          mapping file, index to object name\n        mapping_file_path = os.path.join(model_dir,
          \"index_to_name.json\")\n\n        if os.path.isfile(mapping_file_path):\n            with
          open(mapping_file_path) as f:\n                self.mapping = json.load(f)\n        else:\n            logger.warning(\n                \"Missing
          the index_to_name.json file. Inference output will not include class name.\"\n            )\n\n        self.initialized
          = True\n\n    def preprocess(self, data):\n        \"\"\"Very basic preprocessing
          code - only tokenizes.\n        Extend with your own preprocessing steps
          as needed.\n        \"\"\"\n        print(\"------- input data --------\")\n        print(data)\n        text
          = data[0].get(\"data\")\n        if text is None:\n            text = data[0].get(\"body\")\n\n        logger.info(f\"Received
          text: {text}\")\n\n        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True,
          return_tensors=\"pt\")\n        return inputs\n\n    def inference(self,
          inputs):\n        \"\"\"\n        Predict the class of a text using a trained
          transformer model.\n        \"\"\"\n        # NOTE: This makes the assumption
          that your model expects text to be tokenized\n        # with \"input_ids\"
          and \"token_type_ids\" - which is true for some popular transformer models,
          e.g. bert.\n        # If your transformer model expects different tokenization,
          adapt this code to suit\n        # its expected input format.\n        inputs
          = inputs.to(self.device)\n\n        prediction = self.model(**inputs)[0].argmax().item()\n        logger.info(f\"Model
          predicted: {prediction}\")\n\n        if self.mapping:\n            prediction
          = self.mapping[str(prediction)]\n        return [prediction]\n\n    def
          postprocess(self, inference_output):\n        # TODO: Add any needed post-processing
          of the model predictions here\n        logger.info(f\"Model Name: {self.model.config._name_or_path}\")\n        logger.info(f\"Model
          predicted: {inference_output}\")\n        return inference_output\n\n_service
          = TransformersClassifierHandler()\n\ndef handle(data, context):\n    try:\n        if
          not _service.initialized:\n            _service.initialize(context)\n\n        if
          data is None:\n            return None\n\n        data = _service.preprocess(data)\n        data
          = _service.inference(data)\n        data = _service.postprocess(data)\n\n        return
          data\n    except Exception as e:\n        raise e\n\n    ''''''\n    with
          open(\"pvc/torch_model/handler.py\", \"w\") as f:\n        f.write(x)\n    f.close()\n\n    print(\"Saving
          handler.py complete !!\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Train
          model'', description='''')\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = train_model(**_parsed_args)\n"], "image": "679oose/basepython:1.0"}},
          "name": "Train model"}', pipelines.kubeflow.org/component_ref: '{}'}
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: test-lee
      persistentVolumeClaim: {claimName: lee}
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
