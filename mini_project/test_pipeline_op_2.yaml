apiVersion: argoproj.io/v1alpha1
kind: Workflow
metadata:
  generateName: test-pipeline-
  annotations: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18, pipelines.kubeflow.org/pipeline_compilation_time: '2023-01-19T22:03:17.650205',
    pipelines.kubeflow.org/pipeline_spec: '{"name": "test_pipeline"}'}
  labels: {pipelines.kubeflow.org/kfp_sdk_version: 1.8.18}
spec:
  entrypoint: test-pipeline
  templates:
  - name: load-data
    container:
      args: [--train, /tmp/outputs/train/data, --evaluation, /tmp/outputs/evaluation/data]
      command:
      - sh
      - -c
      - (PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet --no-warn-script-location
        'pandas' || PIP_DISABLE_PIP_VERSION_CHECK=1 python3 -m pip install --quiet
        --no-warn-script-location 'pandas' --user) && "$0" "$@"
      - sh
      - -ec
      - |
        program_path=$(mktemp)
        printf "%s" "$0" > "$program_path"
        python3 -u "$program_path" "$@"
      - |
        def _make_parent_dirs_and_return_path(file_path: str):
            import os
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            return file_path

        def load_data(
            train_path,
            evaluation_path,
        ):

            import pandas as pd

            print("load_pandas")

            # load data from github
            df_train = pd.read_csv(
                "https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/train.csv"
            )
            df_evaluation = pd.read_csv(
                "https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/validation.csv"
            )
            print("Complete_loading_data_to_pandas")

            df_train.to_csv(train_path, index=False)
            df_evaluation.to_csv(evaluation_path, index=False)

            print("complete Loading Data")

        import argparse
        _parser = argparse.ArgumentParser(prog='Load data', description='')
        _parser.add_argument("--train", dest="train_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parser.add_argument("--evaluation", dest="evaluation_path", type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)
        _parsed_args = vars(_parser.parse_args())

        _outputs = load_data(**_parsed_args)
      image: python:3.9
    outputs:
      artifacts:
      - {name: load-data-evaluation, path: /tmp/outputs/evaluation/data}
      - {name: load-data-train, path: /tmp/outputs/train/data}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
      annotations: {pipelines.kubeflow.org/component_spec: '{"implementation": {"container":
          {"args": ["--train", {"outputPath": "train"}, "--evaluation", {"outputPath":
          "evaluation"}], "command": ["sh", "-c", "(PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' || PIP_DISABLE_PIP_VERSION_CHECK=1
          python3 -m pip install --quiet --no-warn-script-location ''pandas'' --user)
          && \"$0\" \"$@\"", "sh", "-ec", "program_path=$(mktemp)\nprintf \"%s\" \"$0\"
          > \"$program_path\"\npython3 -u \"$program_path\" \"$@\"\n", "def _make_parent_dirs_and_return_path(file_path:
          str):\n    import os\n    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n    return
          file_path\n\ndef load_data(\n    train_path,\n    evaluation_path,\n):\n\n    import
          pandas as pd\n\n    print(\"load_pandas\")\n\n    # load data from github\n    df_train
          = pd.read_csv(\n        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/train.csv\"\n    )\n    df_evaluation
          = pd.read_csv(\n        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/validation.csv\"\n    )\n    print(\"Complete_loading_data_to_pandas\")\n\n    df_train.to_csv(train_path,
          index=False)\n    df_evaluation.to_csv(evaluation_path, index=False)\n\n    print(\"complete
          Loading Data\")\n\nimport argparse\n_parser = argparse.ArgumentParser(prog=''Load
          data'', description='''')\n_parser.add_argument(\"--train\", dest=\"train_path\",
          type=_make_parent_dirs_and_return_path, required=True, default=argparse.SUPPRESS)\n_parser.add_argument(\"--evaluation\",
          dest=\"evaluation_path\", type=_make_parent_dirs_and_return_path, required=True,
          default=argparse.SUPPRESS)\n_parsed_args = vars(_parser.parse_args())\n\n_outputs
          = load_data(**_parsed_args)\n"], "image": "python:3.9"}}, "name": "Load
          data", "outputs": [{"name": "train", "type": "csv"}, {"name": "evaluation",
          "type": "csv"}]}', pipelines.kubeflow.org/component_ref: '{}'}
  - name: marfile-test
    container:
      args: [pip install torch-model-archiver torchserve torch-workflow-archiver &&
          ls -al]
      command: [/bin/sh, -c]
      env:
      - {name: MINIO_URL, value: 'http://minio-service.kubeflow.svc.cluster.local:9000'}
      - {name: MINIO_KEY, value: minio}
      - {name: MINIO_SECRET, value: minio123}
      image: python:3.9
      volumeMounts:
      - {mountPath: /mnt, name: volume-creation}
    inputs:
      parameters:
      - {name: volume-creation-name}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
    volumes:
    - name: volume-creation
      persistentVolumeClaim: {claimName: '{{inputs.parameters.volume-creation-name}}'}
  - name: test-pipeline
    dag:
      tasks:
      - {name: load-data, template: load-data}
      - name: marfile-test
        template: marfile-test
        dependencies: [volume-creation]
        arguments:
          parameters:
          - {name: volume-creation-name, value: '{{tasks.volume-creation.outputs.parameters.volume-creation-name}}'}
      - {name: volume-creation, template: volume-creation}
  - name: volume-creation
    resource:
      action: create
      manifest: |
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          name: '{{workflow.name}}-mypvc'
        spec:
          accessModes:
          - ReadWriteMany
          resources:
            requests:
              storage: 1Gi
    outputs:
      parameters:
      - name: volume-creation-manifest
        valueFrom: {jsonPath: '{}'}
      - name: volume-creation-name
        valueFrom: {jsonPath: '{.metadata.name}'}
      - name: volume-creation-size
        valueFrom: {jsonPath: '{.status.capacity.storage}'}
    metadata:
      labels:
        pipelines.kubeflow.org/kfp_sdk_version: 1.8.18
        pipelines.kubeflow.org/pipeline-sdk-type: kfp
        pipelines.kubeflow.org/enable_caching: "true"
  arguments:
    parameters: []
  serviceAccountName: pipeline-runner
