{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFP를 활용해 Training PipeLine 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### New\n",
    "from functools import partial\n",
    "from kfp.components import create_component_from_func, InputPath, OutputPath\n",
    "\n",
    "\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    packages_to_install=[\"pandas\"],\n",
    ")\n",
    "def load_data(\n",
    "    # : OutputPath(\"csv\"),\n",
    "    # evaluation_path: OutputPath(\"csv\"),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    print(\"list_dir : \\n \", os.listdir())\n",
    "\n",
    "    # load data from github\n",
    "    df_train = pd.read_csv(\n",
    "        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/train.csv\"\n",
    "    )\n",
    "    df_evaluation = pd.read_csv(\n",
    "        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/validation.csv\"\n",
    "    )\n",
    "\n",
    "    df_train.to_csv(\"pvc/train.csv\", index=False)\n",
    "    df_evaluation.to_csv(\"pvc/evaluation.csv\", index=False)\n",
    "\n",
    "    print(\"complete Loading Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(create_component_from_func, base_image=\"679oose/basepython:1.0\")\n",
    "def train_model(\n",
    "    # train_path:InputPath(\"csv\"),\n",
    "    # evaluation_path: InputPath(\"csv\"),\n",
    "):\n",
    "\n",
    "    from transformers import (\n",
    "        DistilBertForSequenceClassification,\n",
    "        DistilBertTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        TrainerCallback,\n",
    "    )\n",
    "\n",
    "    from datasets import Dataset\n",
    "\n",
    "    import os\n",
    "\n",
    "    print(\"list_dir : \\n \", os.listdir())\n",
    "    print(\"list_dir : \\n \", os.getcwd())\n",
    "    os.chdir(\"/\")\n",
    "\n",
    "    # loading data\n",
    "    # train_dataset = Dataset.from_csv(train_path).select(range(100))\n",
    "    # evaluation_dataset = Dataset.from_csv(evaluation_path)\n",
    "\n",
    "    train_dataset = Dataset.from_csv(\"pvc/train.csv\").select(range(32))\n",
    "    evaluation_dataset = Dataset.from_csv(\"pvc/evaluation.csv\")\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize_function(item):\n",
    "        return tokenizer(item[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "    train = train_dataset.map(tokenize_function)\n",
    "    evaluation = evaluation_dataset.map(tokenize_function)\n",
    "\n",
    "    print(\"complete Tokenizing\")\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=len(set(train_dataset[\"label\"]))\n",
    "    )\n",
    "\n",
    "    # BetterTransformer\n",
    "\n",
    "    tra_arg = TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        output_dir=\"test\",\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=2,\n",
    "        # evaluation_strategy=\"epoch\",\n",
    "        disable_tqdm=True,\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    class myCallback(TrainerCallback):\n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            print(f\"{state.global_step} Steps \")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=tra_arg,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=evaluation,\n",
    "        callbacks=[myCallback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Saving Tokenizer, Model\n",
    "    trainer.save_model(\"pvc/torch_model\")\n",
    "    tokenizer.save_pretrained(\"pvc/torch_model\")\n",
    "\n",
    "    print(\"Saving Model & Tokenizer Complete !!\")\n",
    "\n",
    "    # config for torch\n",
    "    config = dict(\n",
    "        inference_address=\"http://0.0.0.0:8085\",\n",
    "        management_address=\"http://0.0.0.0:8085\",\n",
    "        metrics_address=\"http://0.0.0.0:8082\",\n",
    "        grpc_inference_port=7070,\n",
    "        grpc_management_port=7071,\n",
    "        enable_envvars_config=\"true\",\n",
    "        install_py_dep_per_model=\"true\",\n",
    "        model_store=\"model-store\",\n",
    "        model_snapshot={\n",
    "            \"name\": \"startup.cfg\",\n",
    "            \"modelCount\": 1,\n",
    "            \"models\": {\n",
    "                \"torch-model\": {  # Model Name\n",
    "                    \"1.0\": {\n",
    "                        \"defaultVersion\": \"true\",\n",
    "                        \"marName\": \"torch-model.mar\",\n",
    "                        \"minWorkers\": 1,\n",
    "                        \"maxWorkers\": 5,\n",
    "                        \"batchSize\": 1,\n",
    "                        \"maxBatchDelay\": 10,\n",
    "                        \"responseTimeout\": 60,\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # making config & config folder\n",
    "    os.mkdir('pvc/torch_model/config')\n",
    "    with open(\"pvc/torch_model/config/config.properties\", \"w\") as f:\n",
    "        for i, j in config.items():\n",
    "            f.write(f\"{i}={j}\\n\")\n",
    "        f.close()\n",
    "\n",
    "    print(\"Saving config.properties !!\")\n",
    "\n",
    "    # handler for torch\n",
    "    x = '''\n",
    "from abc import ABC\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from optimum.bettertransformer import BetterTransformer\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class TransformersClassifierHandler(BaseHandler, ABC):\n",
    "    def __init__(self):\n",
    "        super(TransformersClassifierHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\n",
    "            \"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        # BetterTransformer\n",
    "        self.model = BetterTransformer.transform(self.model)\n",
    "        self.model.eval()\n",
    "\n",
    "        logger.debug(f\"Transformer model from path {model_dir} loaded successfully\")\n",
    "\n",
    "        # Read the mapping file, index to object name\n",
    "        mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n",
    "\n",
    "        if os.path.isfile(mapping_file_path):\n",
    "            with open(mapping_file_path) as f:\n",
    "                self.mapping = json.load(f)\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Missing the index_to_name.json file. Inference output will not include class name.\"\n",
    "            )\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        \"\"\"Very basic preprocessing code - only tokenizes.\n",
    "        Extend with your own preprocessing steps as needed.\n",
    "        \"\"\"\n",
    "        print(\"------- input data --------\")\n",
    "        print(data)\n",
    "        text = data[0].get(\"data\")\n",
    "        if text is None:\n",
    "            text = data[0].get(\"body\")\n",
    "\n",
    "        logger.info(f\"Received text: {text}\")\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        \"\"\"\n",
    "        Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        # NOTE: This makes the assumption that your model expects text to be tokenized\n",
    "        # with \"input_ids\" and \"token_type_ids\" - which is true for some popular transformer models, e.g. bert.\n",
    "        # If your transformer model expects different tokenization, adapt this code to suit\n",
    "        # its expected input format.\n",
    "        inputs = inputs.to(self.device)\n",
    "\n",
    "        prediction = self.model(**inputs)[0].argmax().item()\n",
    "        logger.info(f\"Model predicted: {prediction}\")\n",
    "\n",
    "        if self.mapping:\n",
    "            prediction = self.mapping[str(prediction)]\n",
    "        return [prediction]\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        # TODO: Add any needed post-processing of the model predictions here\n",
    "        logger.info(f\"Model Name: {self.model.config._name_or_path}\")\n",
    "        logger.info(f\"Model predicted: {inference_output}\")\n",
    "        return inference_output\n",
    "\n",
    "\n",
    "_service = TransformersClassifierHandler()\n",
    "\n",
    "\n",
    "def handle(data, context):\n",
    "    try:\n",
    "        if not _service.initialized:\n",
    "            _service.initialize(context)\n",
    "\n",
    "        if data is None:\n",
    "            return None\n",
    "\n",
    "        data = _service.preprocess(data)\n",
    "        data = _service.inference(data)\n",
    "        data = _service.postprocess(data)\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "    '''\n",
    "    with open(\"pvc/torch_model/handler.py\", \"w\") as f:\n",
    "        f.write(x)\n",
    "    f.close()\n",
    "\n",
    "    print(\"Saving handler.py complete !!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import ContainerOp\n",
    "\n",
    "\n",
    "def create_marfile():\n",
    "    return ContainerOp(\n",
    "        name=\"Creating Marfile\",\n",
    "        command=[\"/bin/sh\"],\n",
    "        image=\"python:3.9\",\n",
    "        arguments=[\n",
    "            \"-c\",\n",
    "            \"cd pvc/torch_model; pip install torchserve torch-model-archiver torch-workflow-archiver; torch-model-archiver --model-name torch-model --version 1.0 --serialized-file pytorch_model.bin --handler handler.py --extra-files config.json,vocab.txt --force; mkdir model_store; mv -f torch-model.bar model_store\",\n",
    "        ],  # pip install => create mar file => make model_store => mv marfile to model_store\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    packages_to_install=[\"kserve\"],\n",
    ")\n",
    "def create_inference_model():\n",
    "\n",
    "    from kserve import (\n",
    "        constants,\n",
    "        KServeClient,\n",
    "        V1beta1InferenceService,\n",
    "        V1beta1InferenceServiceSpec,\n",
    "        V1beta1PredictorSpec,\n",
    "        V1beta1ModelSpec,\n",
    "        V1beta1ModelFormat,\n",
    "        V1beta1TorchServeSpec,\n",
    "        utils,\n",
    "    )\n",
    "    from kubernetes import client\n",
    "\n",
    "    client.V1ResourceRequirements(requests={\"cpu\": 1, \"memory\": \"1G\"})\n",
    "\n",
    "    service_name = \"pytorchserve2\"\n",
    "    namespace = utils.get_default_target_namespace()\n",
    "    api_version = constants.KSERVE_GROUP + \"/\" + constants.KSERVE_V1BETA1_VERSION\n",
    "    storage = \"pvc://lee/torch_model\"\n",
    "\n",
    "    torchsvc = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=service_name, namespace=namespace, annotations={\"sidecar.istio.io/inject\": \"false\"}\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                pytorch=(\n",
    "                    V1beta1TorchServeSpec(\n",
    "                        protocol_version=\"v1\",\n",
    "                        resources=client.V1ResourceRequirements(\n",
    "                            requests={\"cpu\": 1, \"memory\": \"1G\"}\n",
    "                        ),\n",
    "                        storage_uri=storage,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(torchsvc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangwoolee/.pyenv/versions/3.9.1/lib/python3.9/site-packages/kfp/dsl/_container_op.py:1261: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 30] Read-only file system: 'NLP_Pipeline.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m/Users/yangwoolee/git_repo/Learning_kubeflow/mini_project/0.pipeline.ipynb 셀 6\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Learning_kubeflow/mini_project/0.pipeline.ipynb#W4sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mkfp\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Learning_kubeflow/mini_project/0.pipeline.ipynb#W4sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yangwoolee/git_repo/Learning_kubeflow/mini_project/0.pipeline.ipynb#W4sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     kfp\u001b[39m.\u001b[39;49mcompiler\u001b[39m.\u001b[39;49mCompiler()\u001b[39m.\u001b[39;49mcompile(NLP_Pipeline, \u001b[39m\"\u001b[39;49m\u001b[39mNLP_Pipeline.yaml\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/kfp/compiler/compiler.py:1175\u001b[0m, in \u001b[0;36mCompiler.compile\u001b[0;34m(self, pipeline_func, package_path, type_check, pipeline_conf)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1174\u001b[0m     kfp\u001b[39m.\u001b[39mTYPE_CHECK \u001b[39m=\u001b[39m type_check\n\u001b[0;32m-> 1175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_and_write_workflow(\n\u001b[1;32m   1176\u001b[0m         pipeline_func\u001b[39m=\u001b[39;49mpipeline_func,\n\u001b[1;32m   1177\u001b[0m         pipeline_conf\u001b[39m=\u001b[39;49mpipeline_conf,\n\u001b[1;32m   1178\u001b[0m         package_path\u001b[39m=\u001b[39;49mpackage_path)\n\u001b[1;32m   1179\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1180\u001b[0m     kfp\u001b[39m.\u001b[39mTYPE_CHECK \u001b[39m=\u001b[39m type_check_old_value\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/kfp/compiler/compiler.py:1230\u001b[0m, in \u001b[0;36mCompiler._create_and_write_workflow\u001b[0;34m(self, pipeline_func, pipeline_name, pipeline_description, params_list, pipeline_conf, package_path)\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[39m\"\"\"Compile the given pipeline function and dump it to specified file\u001b[39;00m\n\u001b[1;32m   1226\u001b[0m \u001b[39mformat.\"\"\"\u001b[39;00m\n\u001b[1;32m   1227\u001b[0m workflow \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_workflow(pipeline_func, pipeline_name,\n\u001b[1;32m   1228\u001b[0m                                  pipeline_description, params_list,\n\u001b[1;32m   1229\u001b[0m                                  pipeline_conf)\n\u001b[0;32m-> 1230\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_write_workflow(workflow, package_path)\n\u001b[1;32m   1231\u001b[0m _validate_workflow(workflow)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.9.1/lib/python3.9/site-packages/kfp/compiler/compiler.py:1211\u001b[0m, in \u001b[0;36mCompiler._write_workflow\u001b[0;34m(workflow, package_path)\u001b[0m\n\u001b[1;32m   1209\u001b[0m         \u001b[39mzip\u001b[39m\u001b[39m.\u001b[39mwritestr(zipinfo, yaml_text)\n\u001b[1;32m   1210\u001b[0m \u001b[39melif\u001b[39;00m package_path\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.yaml\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m package_path\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.yml\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m-> 1211\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(package_path, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m yaml_file:\n\u001b[1;32m   1212\u001b[0m         yaml_file\u001b[39m.\u001b[39mwrite(yaml_text)\n\u001b[1;32m   1213\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 30] Read-only file system: 'NLP_Pipeline.yaml'"
     ]
    }
   ],
   "source": [
    "from kfp.dsl import pipeline\n",
    "from kfp import onprem\n",
    "from kfp.dsl import ContainerOp\n",
    "\n",
    "@pipeline(name=\"NLP_Pipeline\")\n",
    "def NLP_Pipeline():\n",
    "    data = load_data()\n",
    "    data.apply(onprem.mount_pvc(pvc_name='lee', volume_name='test-lee', volume_mount_path=\"pvc\"))\n",
    "    # model = train_model(data.outputs['train'],data.outputs['evaluation'])    \n",
    "    model = train_model()\n",
    "    model.apply(onprem.mount_pvc(pvc_name='lee', volume_name='test-lee', volume_mount_path=\"pvc\"))\n",
    "    model.set_cpu_limit(cpu = '2').set_memory_limit(memory = '4G')\n",
    "    model.set_display_name('Finetuning Text Classification Model')\n",
    "    model.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # 매번 새롭게 시작하게 하는 명령어\n",
    "    model.after(data)\n",
    "    \n",
    "    marfile=create_marfile()\n",
    "    marfile.apply(onprem.mount_pvc(pvc_name='lee', volume_name='test-lee', volume_mount_path=\"pvc\"))\n",
    "    marfile.set_display_name('Creating Marfile')\n",
    "    marfile.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # 매번 새롭게 시작하게 하는 명령어\n",
    "    marfile.after(model)\n",
    "\n",
    "    inference_model=create_inference_model()\n",
    "    inference_model.apply(onprem.mount_pvc(pvc_name='lee', volume_name='test-lee', volume_mount_path=\"pvc\"))\n",
    "    inference_model.after(marfile)\n",
    "\n",
    "import kfp\n",
    "if __name__ == \"__main__\":\n",
    "    kfp.compiler.Compiler().compile(NLP_Pipeline, \"NLP_Pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp import Client\n",
    "\n",
    "Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
