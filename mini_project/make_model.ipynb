{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습 평가 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/yangwoolee/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-a0e621c27d9b360e.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/yangwoolee/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-61e0da4d9cd46a2c.arrow\n",
      "Loading cached shuffled indices for dataset at /Users/yangwoolee/.cache/huggingface/datasets/yelp_review_full/yelp_review_full/1.0.0/e8e18e19d7be9e75642fc66b198abadb116f73599ec89a69ba5dd8d1e57ba0bf/cache-61e0da4d9cd46a2c.arrow\n"
     ]
    }
   ],
   "source": [
    "train_dataset = dataset[\"train\"].shuffle(seed=42).select([i for i in list(range(1000))])\n",
    "validation_dataset = dataset[\"test\"].shuffle(seed=42).select([i for i in list(range(100))])\n",
    "test_dataset = dataset[\"test\"].shuffle(seed=42).select([i for i in list(range(50))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7eea51da8514af2964b65ce443c1798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c0f3aa8ae16419c958303c29365da9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "956c4eacb6e94da4a3a1143fc28374bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "36382"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_csv('data/train.csv',index=False)\n",
    "validation_dataset.to_csv('data/validation.csv',index=False)\n",
    "test_dataset.to_csv('data/test.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-791619f85ce504b1\n",
      "/Users/yangwoolee/.pyenv/versions/3.9.1/lib/python3.9/site-packages/datasets/builder.py:712: FutureWarning: 'use_auth_token' was deprecated in version 2.7.1 and will be removed in 3.0.0. Pass `use_auth_token` to the initializer/`load_dataset_builder` instead.\n",
      "  warnings.warn(\n",
      "Found cached dataset csv (/Users/yangwoolee/.cache/huggingface/datasets/csv/default-791619f85ce504b1/0.0.0)\n",
      "Using custom data configuration default-77a87d425e9ea9ea\n",
      "Found cached dataset csv (/Users/yangwoolee/.cache/huggingface/datasets/csv/default-77a87d425e9ea9ea/0.0.0)\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "        DistilBertForSequenceClassification,\n",
    "        DistilBertTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        TrainerCallback\n",
    "    )\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "# loading data\n",
    "train_path = 'data/train.csv'\n",
    "evaluation_path = 'data/validation.csv'\n",
    "train_dataset = Dataset.from_csv(train_path).select(range(8))\n",
    "evaluation_dataset = Dataset.from_csv(evaluation_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /Users/yangwoolee/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/yangwoolee/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/yangwoolee/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /Users/yangwoolee/.cache/huggingface/datasets/csv/default-791619f85ce504b1/0.0.0/cache-76190b5a576e2a78.arrow\n",
      "Loading cached processed dataset at /Users/yangwoolee/.cache/huggingface/datasets/csv/default-77a87d425e9ea9ea/0.0.0/cache-202a62494a7b7709.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete Tokenizing\n"
     ]
    }
   ],
   "source": [
    "# tokenizing\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "def tokenize_function(item):\n",
    "    return tokenizer(item[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "train = train_dataset.map(tokenize_function)\n",
    "evaluation = evaluation_dataset.map(tokenize_function)\n",
    "\n",
    "print('complete Tokenizing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': [4], 'text': [\"I stalk this truck.  I've been to industrial parks where I pretend to be a tech worker standing in line, strip mall parking lots, and of course the farmer's market.  The bowls are so so absolutely divine.  The owner is super friendly and he makes each bowl by hand with an incredible amount of pride.  You gotta eat here guys!!!\"], 'input_ids': [[101, 1045, 23899, 2023, 4744, 1012, 1045, 1005, 2310, 2042, 2000, 3919, 6328, 2073, 1045, 9811, 2000, 2022, 1037, 6627, 7309, 3061, 1999, 2240, 1010, 6167, 6670, 5581, 7167, 1010, 1998, 1997, 2607, 1996, 7500, 1005, 1055, 3006, 1012, 1996, 15220, 2024, 2061, 2061, 7078, 7746, 1012, 1996, 3954, 2003, 3565, 5379, 1998, 2002, 3084, 2169, 4605, 2011, 2192, 2007, 2019, 9788, 3815, 1997, 6620, 1012, 2017, 10657, 4521, 2182, 4364, 999, 999, 999, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
     ]
    }
   ],
   "source": [
    "print(train[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased\", num_labels=5\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilbert-base-uncased'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config._name_or_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tra_arg = TrainingArguments(\n",
    "    num_train_epochs=1,\n",
    "    output_dir=\"test\",\n",
    "    logging_steps=10,\n",
    "    # evaluation_strategy=\"epoch\",\n",
    ")\n",
    "\n",
    "class myCallback(TrainerCallback):\n",
    "\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        print(f'{state.global_step}회 진행 중 ')\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=tra_arg,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=evaluation,\n",
    "    callbacks=[myCallback]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in test123/tokenizer_config.json\n",
      "Special tokens file saved in test123/special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('test123/tokenizer_config.json',\n",
       " 'test123/special_tokens_map.json',\n",
       " 'test123/vocab.txt',\n",
       " 'test123/added_tokens.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained('test123')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KFP를 활용해 Training PipeLine 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### New\n",
    "from functools import partial\n",
    "from kfp.components import create_component_from_func, InputPath, OutputPath\n",
    "\n",
    "\n",
    "\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    packages_to_install=[\"pandas\"],\n",
    ")\n",
    "def load_data(\n",
    "    train_path: OutputPath(\"csv\"),\n",
    "    evaluation_path: OutputPath(\"csv\"),\n",
    "):\n",
    "\n",
    "\n",
    "    import pandas as pd\n",
    "\n",
    "    # load data from github\n",
    "    df_train = pd.read_csv(\n",
    "        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/train.csv\"\n",
    "    )\n",
    "    df_evaluation = pd.read_csv(\n",
    "        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/validation.csv\"\n",
    "    )\n",
    "    \n",
    "    df_train.to_csv(train_path, index=False)\n",
    "    df_evaluation.to_csv(evaluation_path, index=False)\n",
    "\n",
    "    print('complete Loading Data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    base_image=\"679oose/basepython:1.0\"\n",
    ")\n",
    "def train_model(\n",
    "    train_path:InputPath(\"csv\"),\n",
    "    evaluation_path: InputPath(\"csv\"),\n",
    "    model_save_path: OutputPath(\"folder\"),\n",
    "):\n",
    "\n",
    "    from transformers import (\n",
    "        DistilBertForSequenceClassification,\n",
    "        DistilBertTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        TrainerCallback\n",
    "    )\n",
    "    from datasets import Dataset\n",
    "\n",
    "    # loading data\n",
    "    train_dataset = Dataset.from_csv(train_path)\n",
    "    evaluation_dataset = Dataset.from_csv(evaluation_path)\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize_function(item):\n",
    "        return tokenizer(item[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "    train = train_dataset.map(tokenize_function)\n",
    "    evaluation = evaluation_dataset.map(tokenize_function)\n",
    "\n",
    "    print('complete Tokenizing')\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=len(set(train_dataset[\"label\"]))\n",
    "    )\n",
    "    tra_arg = TrainingArguments(\n",
    "        output_dir=\"test\",\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=5,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        disable_tqdm=True,\n",
    "        save_strategy = \"no\"\n",
    "    )\n",
    "\n",
    "    class myCallback(TrainerCallback):\n",
    "\n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            print(f'{state.global_step} Steps ')\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=tra_arg,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=evaluation,\n",
    "        callbacks=[myCallback]\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    trainer.save_model(model_save_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import pipeline\n",
    "\n",
    "@pipeline(name=\"NLP_Pipeline\")\n",
    "def NLP_Pipeline():\n",
    "    data = load_data()\n",
    "    train_model(data.outputs['train'],data.outputs['evaluation'])\n",
    "\n",
    "\n",
    "import kfp\n",
    "if __name__ == \"__main__\":\n",
    "    kfp.compiler.Compiler().compile(NLP_Pipeline, \"NLP_Pipeline_1.2.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
