{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangwoolee/.pyenv/versions/3.9.1/lib/python3.9/site-packages/urllib3/connection.py:458: SubjectAltNameWarning: Certificate for localhost has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/urllib3/urllib3/issues/497 for details.)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "접속 :  {'multi_user': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangwoolee/.pyenv/versions/3.9.1/lib/python3.9/site-packages/urllib3/connection.py:458: SubjectAltNameWarning: Certificate for localhost has no `subjectAltName`, falling back to check for a `commonName` for now. This feature is being removed by major browsers and deprecated by RFC 2818. (See https://github.com/urllib3/urllib3/issues/497 for details.)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import requests\n",
    "from urllib.parse import urlsplit\n",
    "import os\n",
    "\n",
    "os.environ[\n",
    "    \"cert_for_kubeflow\"\n",
    "] = \"/Users/yangwoolee/git_repo/Learning_kubeflow/manifests/deployment/cert/leeway.crt\"\n",
    "\n",
    "\n",
    "\n",
    "def get_istio_auth_session(url: str, username: str, password: str) -> dict:\n",
    "    \"\"\"\n",
    "    Determine if the specified URL is secured by Dex and try to obtain a session cookie.\n",
    "    WARNING: only Dex `staticPasswords` and `LDAP` authentication are currently supported\n",
    "             (we default default to using `staticPasswords` if both are enabled)\n",
    "\n",
    "    :param url: Kubeflow server URL, including protocol\n",
    "    :param username: Dex `staticPasswords` or `LDAP` username\n",
    "    :param password: Dex `staticPasswords` or `LDAP` password\n",
    "    :return: auth session information\n",
    "    \"\"\"\n",
    "    # define the default return object\n",
    "    auth_session = {\n",
    "        \"endpoint_url\": url,  # KF endpoint URL\n",
    "        \"redirect_url\": None,  # KF redirect URL, if applicable\n",
    "        \"dex_login_url\": None,  # Dex login URL (for POST of credentials)\n",
    "        \"is_secured\": None,  # True if KF endpoint is secured\n",
    "        \"session_cookie\": None,  # Resulting session cookies in the form \"key1=value1; key2=value2\"\n",
    "    }\n",
    "\n",
    "    # use a persistent session (for cookies)\n",
    "    with requests.Session() as s:\n",
    "\n",
    "        ################\n",
    "        # Determine if Endpoint is Secured\n",
    "        ################\n",
    "        resp = s.get(url, allow_redirects=True, verify=os.getenv(\"cert_for_kubeflow\"))\n",
    "        if resp.status_code != 200:\n",
    "            raise RuntimeError(f\"HTTP status code '{resp.status_code}' for GET against: {url}\")\n",
    "\n",
    "        auth_session[\"redirect_url\"] = resp.url\n",
    "\n",
    "        # if we were NOT redirected, then the endpoint is UNSECURED\n",
    "        if len(resp.history) == 0:\n",
    "            auth_session[\"is_secured\"] = False\n",
    "            return auth_session\n",
    "        else:\n",
    "            auth_session[\"is_secured\"] = True\n",
    "\n",
    "        ################\n",
    "        # Get Dex Login URL\n",
    "        ################\n",
    "        redirect_url_obj = urlsplit(auth_session[\"redirect_url\"])\n",
    "\n",
    "        # if we are at `/auth?=xxxx` path, we need to select an auth type\n",
    "        if re.search(r\"/auth$\", redirect_url_obj.path):\n",
    "\n",
    "            #######\n",
    "            # TIP: choose the default auth type by including ONE of the following\n",
    "            #######\n",
    "\n",
    "            # OPTION 1: set \"staticPasswords\" as default auth type\n",
    "            redirect_url_obj = redirect_url_obj._replace(\n",
    "                path=re.sub(r\"/auth$\", \"/auth/local\", redirect_url_obj.path)\n",
    "            )\n",
    "            # OPTION 2: set \"ldap\" as default auth type\n",
    "            # redirect_url_obj = redirect_url_obj._replace(\n",
    "            #     path=re.sub(r\"/auth$\", \"/auth/ldap\", redirect_url_obj.path)\n",
    "            # )\n",
    "\n",
    "        # if we are at `/auth/xxxx/login` path, then no further action is needed (we can use it for login POST)\n",
    "        if re.search(r\"/auth/.*/login$\", redirect_url_obj.path):\n",
    "            auth_session[\"dex_login_url\"] = redirect_url_obj.geturl()\n",
    "\n",
    "        # else, we need to be redirected to the actual login page\n",
    "        else:\n",
    "            # this GET should redirect us to the `/auth/xxxx/login` path\n",
    "            resp = s.get(redirect_url_obj.geturl(), allow_redirects=True)\n",
    "            if resp.status_code != 200:\n",
    "                raise RuntimeError(\n",
    "                    f\"HTTP status code '{resp.status_code}' for GET against: {redirect_url_obj.geturl()}\"\n",
    "                )\n",
    "\n",
    "            # set the login url\n",
    "            auth_session[\"dex_login_url\"] = resp.url\n",
    "\n",
    "        ################\n",
    "        # Attempt Dex Login\n",
    "        ################\n",
    "        resp = s.post(\n",
    "            auth_session[\"dex_login_url\"],\n",
    "            data={\"login\": username, \"password\": password},\n",
    "            allow_redirects=True,\n",
    "        )\n",
    "        if len(resp.history) == 0:\n",
    "            raise RuntimeError(\n",
    "                f\"Login credentials were probably invalid - \"\n",
    "                f\"No redirect after POST to: {auth_session['dex_login_url']}\"\n",
    "            )\n",
    "\n",
    "        # store the session cookies in a \"key1=value1; key2=value2\" string\n",
    "        auth_session[\"session_cookie\"] = \"; \".join([f\"{c.name}={c.value}\" for c in s.cookies])\n",
    "    return auth_session\n",
    "\n",
    "\n",
    "import kfp\n",
    "import os\n",
    "\n",
    "KUBEFLOW_ENDPOINT = \"https://localhost:8080\"\n",
    "KUBEFLOW_USERNAME = \"user@example.com\"\n",
    "KUBEFLOW_PASSWORD = \"12341234\"\n",
    "\n",
    "auth_session = get_istio_auth_session(\n",
    "    url=KUBEFLOW_ENDPOINT, username=KUBEFLOW_USERNAME, password=KUBEFLOW_PASSWORD\n",
    ")\n",
    "client = kfp.Client(\n",
    "    host=f\"{KUBEFLOW_ENDPOINT}/pipeline\",\n",
    "    cookies=auth_session[\"session_cookie\"],\n",
    "    ssl_ca_cert=os.getenv(\"cert_for_kubeflow\"),\n",
    ")\n",
    "print(\"접속 : \", client.get_kfp_healthz())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from kfp.components import create_component_from_func,load_component_from_url\n",
    "\n",
    "\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    packages_to_install=[\"pandas\"],\n",
    ")\n",
    "def load_data(\n",
    "    # : OutputPath(\"csv\"),\n",
    "    # evaluation_path: OutputPath(\"csv\"),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    print(\"list_dir : \\n \", os.listdir())\n",
    "\n",
    "    # load data from github\n",
    "    df_train = pd.read_csv(\n",
    "        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/train.csv\"\n",
    "    )\n",
    "    df_evaluation = pd.read_csv(\n",
    "        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/validation.csv\"\n",
    "    )\n",
    "\n",
    "    df_train.to_csv(\"pvc/train.csv\", index=False)\n",
    "    df_evaluation.to_csv(\"pvc/evaluation.csv\", index=False)\n",
    "\n",
    "    print(\"complete Loading Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(create_component_from_func, base_image=\"679oose/python_huggingface\")\n",
    "def train_model(\n",
    "    # train_path:InputPath(\"csv\"),\n",
    "    # evaluation_path: InputPath(\"csv\"),\n",
    "):\n",
    "\n",
    "    from transformers import (\n",
    "        DistilBertForSequenceClassification,\n",
    "        DistilBertTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        TrainerCallback,\n",
    "    )\n",
    "\n",
    "    from datasets import Dataset\n",
    "\n",
    "    import os\n",
    "\n",
    "    print(\"list_dir : \\n \", os.listdir())\n",
    "    print(\"list_dir : \\n \", os.getcwd())\n",
    "    os.chdir(\"/\")\n",
    "\n",
    "    # loading data\n",
    "    # train_dataset = Dataset.from_csv(train_path).select(range(100))\n",
    "    # evaluation_dataset = Dataset.from_csv(evaluation_path)\n",
    "\n",
    "    train_dataset = Dataset.from_csv(\"pvc/train.csv\").select(range(32))\n",
    "    evaluation_dataset = Dataset.from_csv(\"pvc/evaluation.csv\")\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize_function(item):\n",
    "        return tokenizer(item[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "    train = train_dataset.map(tokenize_function)\n",
    "    evaluation = evaluation_dataset.map(tokenize_function)\n",
    "\n",
    "    print(\"complete Tokenizing\")\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=len(set(train_dataset[\"label\"]))\n",
    "    )\n",
    "\n",
    "    # BetterTransformer\n",
    "\n",
    "    tra_arg = TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        output_dir=\"test\",\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=2,\n",
    "        # evaluation_strategy=\"epoch\",\n",
    "        disable_tqdm=True,\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    class myCallback(TrainerCallback):\n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            print(f\"{state.global_step} Steps \")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=tra_arg,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=evaluation,\n",
    "        callbacks=[myCallback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Saving Tokenizer, Model\n",
    "    trainer.save_model(\"pvc/torch_model\")\n",
    "    tokenizer.save_pretrained(\"pvc/torch_model\")\n",
    "\n",
    "    print(\"Saving Model & Tokenizer Complete !!\")\n",
    "\n",
    "    # config for torch\n",
    "    import json\n",
    "    config = dict(\n",
    "        inference_address=\"http://0.0.0.0:8085\",\n",
    "        management_address=\"http://0.0.0.0:8085\",\n",
    "        metrics_address=\"http://0.0.0.0:8082\",\n",
    "        grpc_inference_port=7070,\n",
    "        grpc_management_port=7071,\n",
    "        enable_envvars_config=\"true\",\n",
    "        install_py_dep_per_model=\"true\",\n",
    "        model_store=\"model-store\",\n",
    "        model_snapshot=json.dumps({\n",
    "            \"name\": \"startup.cfg\",\n",
    "            \"modelCount\": 1,\n",
    "            \"models\": {\n",
    "                \"torch-model\": {  # Model Name\n",
    "                    \"1.0\": {\n",
    "                        \"defaultVersion\": \"true\",\n",
    "                        \"marName\": \"torch-model.mar\",\n",
    "                        \"minWorkers\": 1,\n",
    "                        \"maxWorkers\": 5,\n",
    "                        \"batchSize\": 1,\n",
    "                        \"maxBatchDelay\": 10,\n",
    "                        \"responseTimeout\": 60,\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }),\n",
    "    )\n",
    "\n",
    "    # making config & config folder\n",
    "    if not os.path.exists(\"pvc/torch_model/config\"):\n",
    "        os.mkdir(\"pvc/torch_model/config\")\n",
    "\n",
    "    with open(\"pvc/torch_model/config/config.properties\", \"w\") as f:\n",
    "        for i, j in config.items():\n",
    "            f.write(f\"{i}={j}\\n\")\n",
    "        f.close()\n",
    "\n",
    "    print(\"Saving config.properties !!\")\n",
    "\n",
    "    # handler for torch\n",
    "    x = '''\n",
    "from abc import ABC\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TransformersClassifierHandler(BaseHandler, ABC):\n",
    "    def __init__(self):\n",
    "        super(TransformersClassifierHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\n",
    "            \"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        # BetterTransformer\n",
    "        self.model.eval()\n",
    "\n",
    "        logger.debug(f\"Transformer model from path {model_dir} loaded successfully\")\n",
    "\n",
    "        # Read the mapping file, index to object name\n",
    "        mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n",
    "\n",
    "        if os.path.isfile(mapping_file_path):\n",
    "            with open(mapping_file_path) as f:\n",
    "                self.mapping = json.load(f)\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Missing the index_to_name.json file. Inference output will not include class name.\"\n",
    "            )\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        \"\"\"Very basic preprocessing code - only tokenizes.\n",
    "        Extend with your own preprocessing steps as needed.\n",
    "        \"\"\"\n",
    "        print(\"------- input data --------\")\n",
    "        print(data)\n",
    "        text = data[0].get(\"data\")\n",
    "        if text is None:\n",
    "            text = data[0].get(\"body\")\n",
    "\n",
    "        logger.info(f\"Received text: {text}\")\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        \"\"\"\n",
    "        Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        # NOTE: This makes the assumption that your model expects text to be tokenized\n",
    "        # with \"input_ids\" and \"token_type_ids\" - which is true for some popular transformer models, e.g. bert.\n",
    "        # If your transformer model expects different tokenization, adapt this code to suit\n",
    "        # its expected input format.\n",
    "        inputs = inputs.to(self.device)\n",
    "\n",
    "        prediction = self.model(**inputs)[0].argmax().item()\n",
    "        logger.info(f\"Model predicted: {prediction}\")\n",
    "\n",
    "        if self.mapping:\n",
    "            prediction = self.mapping[str(prediction)]\n",
    "        return [prediction]\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        # TODO: Add any needed post-processing of the model predictions here\n",
    "        logger.info(f\"Model Name: {self.model.config._name_or_path}\")\n",
    "        logger.info(f\"Model predicted: {inference_output}\")\n",
    "        return inference_output\n",
    "\n",
    "\n",
    "_service = TransformersClassifierHandler()\n",
    "\n",
    "\n",
    "def handle(data, context):\n",
    "    try:\n",
    "        if not _service.initialized:\n",
    "            _service.initialize(context)\n",
    "\n",
    "        if data is None:\n",
    "            return None\n",
    "\n",
    "        data = _service.preprocess(data)\n",
    "        data = _service.inference(data)\n",
    "        data = _service.postprocess(data)\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "\n",
    "    '''\n",
    "    with open(\"pvc/torch_model/handler.py\", \"w\") as f:\n",
    "        f.write(x)\n",
    "    f.close()\n",
    "\n",
    "    print(\"Saving handler.py complete !!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import ContainerOp\n",
    "\n",
    "\n",
    "def create_marfile():\n",
    "    return ContainerOp(\n",
    "        name=\"Creating Marfile\",\n",
    "        command=[\"/bin/sh\"],\n",
    "        image=\"python:3.9\",\n",
    "        arguments=[\n",
    "            \"-c\",\n",
    "            \"cd pvc/torch_model; pip install torchserve torch-model-archiver torch-workflow-archiver; torch-model-archiver --model-name torch-model --version 1.0 --serialized-file pytorch_model.bin --handler handler.py --extra-files config.json,vocab.txt --force; mkdir model-store; mv -f torch-model.mar model-store\"\n",
    "        ],  # pip install => create mar file => make model_store folder => mv marfile to model_store\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @partial(create_component_from_func, base_image=\"679oose/python_kserve_0.9.0\")\n",
    "# def create_inference_model():\n",
    "\n",
    "#     from kserve import (\n",
    "#         constants,\n",
    "#         KServeClient,\n",
    "#         V1beta1InferenceService,\n",
    "#         V1beta1InferenceServiceSpec,\n",
    "#         V1beta1PredictorSpec,\n",
    "#         V1beta1ModelSpec,\n",
    "#         V1beta1ModelFormat,\n",
    "#         V1beta1TorchServeSpec,\n",
    "#         utils,\n",
    "#     )\n",
    "#     from kubernetes import client\n",
    "\n",
    "#     service_name = \"torchserve\"\n",
    "#     namespace = \"kubeflow-user-example-com\"\n",
    "#     api_version = constants.KSERVE_GROUP + \"/\" + constants.KSERVE_V1BETA1_VERSION\n",
    "#     storage = \"pvc://leeway/torch_model\"\n",
    "\n",
    "#     torchsvc = V1beta1InferenceService(\n",
    "#         api_version=api_version,\n",
    "#         kind=constants.KSERVE_KIND,\n",
    "#         metadata=client.V1ObjectMeta(name=service_name, namespace=namespace, annotations={\"sidecar.istio.io/inject\": \"false\"}),\n",
    "#         spec=V1beta1InferenceServiceSpec(\n",
    "#             predictor=V1beta1PredictorSpec(\n",
    "#                 pytorch=(\n",
    "#                     V1beta1TorchServeSpec(\n",
    "#                         protocol_version=\"v1\",\n",
    "#                         resources=client.V1ResourceRequirements(\n",
    "#                             requests={\"cpu\": 1, \"memory\": \"1G\"}\n",
    "#                         ),\n",
    "#                         storage_uri=storage,\n",
    "#                     )\n",
    "#                 )\n",
    "#             )\n",
    "#         ),\n",
    "#     )\n",
    "#     KServe = KServeClient()\n",
    "#     KServe.create(torchsvc)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_inference_model():\n",
    "    kserve_op = load_component_from_url('https://raw.githubusercontent.com/kubeflow/pipelines/'\n",
    "                                               'master/components/kserve/component.yaml')\n",
    "\n",
    "    model_name = \"torchserve\"\n",
    "    namespace = \"kubeflow-user-example-com\"\n",
    "    # api_version = constants.KSERVE_GROUP + \"/\" + constants.KSERVE_V1BETA1_VERSION\n",
    "    model_uri = \"pvc://leeway/torch_model\"\n",
    "    framework=\"pytorch\"\n",
    "\n",
    "    return kserve_op(action=\"apply\",\n",
    "              model_name=model_name,\n",
    "              model_uri=model_uri,\n",
    "              namespace=namespace,\n",
    "              framework=framework)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangwoolee/.pyenv/versions/3.9.1/lib/python3.9/site-packages/kfp/dsl/_container_op.py:1261: FutureWarning: Please create reusable components instead of constructing ContainerOp instances directly. Reusable components are shareable, portable and have compatibility and support guarantees. Please see the documentation: https://www.kubeflow.org/docs/pipelines/sdk/component-development/#writing-your-component-definition-file The components can be created manually (or, in case of python, using kfp.components.create_component_from_func or func_to_container_op) and then loaded using kfp.components.load_component_from_file, load_component_from_uri or load_component_from_text: https://kubeflow-pipelines.readthedocs.io/en/stable/source/kfp.components.html#kfp.components.load_component_from_file\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://localhost:8080/pipeline/#/experiments/details/43133351-e045-47e8-a175-fe31d81e12a1\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"https://localhost:8080/pipeline/#/runs/details/49f3fbaf-d1a4-4d5b-b437-8cdd62da02c5\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from kfp.dsl import ContainerOp, pipeline\n",
    "from kfp import onprem\n",
    "\n",
    "\n",
    "@pipeline(name=\"NLP_Pipeline\")\n",
    "def NLP_Pipeline():\n",
    "    data = load_data()\n",
    "    data.apply(onprem.mount_pvc(pvc_name=\"leeway\", volume_name=\"test-lee\", volume_mount_path=\"pvc\"))\n",
    "\n",
    "    model = train_model()\n",
    "    model.apply(\n",
    "        onprem.mount_pvc(pvc_name=\"leeway\", volume_name=\"test-lee\", volume_mount_path=\"pvc\")\n",
    "    )\n",
    "    model.set_cpu_limit(cpu=\"1\").set_memory_limit(memory=\"4G\")\n",
    "    model.set_display_name(\"Finetuning Text Classification Model\")\n",
    "    # model.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # cache 사용하지 않는 명령어\n",
    "    model.after(data)\n",
    "\n",
    "    marfile = create_marfile()\n",
    "    marfile.apply(\n",
    "        onprem.mount_pvc(pvc_name=\"leeway\", volume_name=\"test-lee\", volume_mount_path=\"pvc\")\n",
    "    )\n",
    "    marfile.set_display_name(\"Creating Marfile\")\n",
    "    marfile.execution_options.caching_strategy.max_cache_staleness = \"P0D\" \n",
    "    marfile.after(model)\n",
    "\n",
    "    inference_model = create_inference_model()\n",
    "    inference_model.apply(\n",
    "        onprem.mount_pvc(pvc_name=\"leeway\", volume_name=\"test-lee\", volume_mount_path=\"pvc\")\n",
    "    )\n",
    "    # inference_model.execution_options.caching_strategy.max_cache_staleness = (\"P0D\")\n",
    "    inference_model.after(marfile)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    client.create_run_from_pipeline_func(\n",
    "        NLP_Pipeline, arguments={}, namespace=\"kubeflow-user-example-com\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MTY3NDcyNjg1MHxOd3dBTkZJMFMwSklRVk5PU0ZsVlVUVldVVWMyTmxsRFQxbFJTME5DVFZOQlVsaEdNMDFYVmxwTVZrdEpTMUJGUkV4U1VVUXlSRkU9fBK2yV1Tfjic8Q_xq5S9-Tg-0z8kaiV6cnRCEAyec9Nc'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yangwoolee/.pyenv/versions/3.9.1/lib/python3.9/site-packages/urllib3/connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'localhost'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"predictions\": [4]}'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torchserve_name = \"torch-model\"\n",
    "model_name = \"pytorchserve\"\n",
    "url = f\"https://localhost:8080/v1/models/{torchserve_name}:predict\"\n",
    "host = f\"{model_name}.kubeflow-user-example-com.example.com\"\n",
    "session={'authservice_session':auth_session[\"session_cookie\"].replace('authservice_session=','')}\n",
    "data = {\"instances\": [{\"data\": \"Hello World!\"}]}  # data\n",
    "\n",
    "x = requests.post(\n",
    "    url=url, verify=False, cookies=session, headers={\"Host\": host}, json=data\n",
    ")\n",
    "x.text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
