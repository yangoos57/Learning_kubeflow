{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### New\n",
    "from functools import partial\n",
    "from kfp.components import create_component_from_func, InputPath, OutputPath\n",
    "\n",
    "\n",
    "@partial(\n",
    "    create_component_from_func,\n",
    "    packages_to_install=[\"pandas\"],\n",
    ")\n",
    "def load_data(\n",
    "    # : OutputPath(\"csv\"),\n",
    "    # evaluation_path: OutputPath(\"csv\"),\n",
    "):\n",
    "\n",
    "    import pandas as pd\n",
    "    import os\n",
    "\n",
    "    print(\"list_dir : \\n \", os.listdir())\n",
    "\n",
    "    # load data from github\n",
    "    df_train = pd.read_csv(\n",
    "        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/train.csv\"\n",
    "    )\n",
    "    df_evaluation = pd.read_csv(\n",
    "        \"https://raw.github.com/yangoos57/Learning_kubeflow/main/mini_project/data/validation.csv\"\n",
    "    )\n",
    "\n",
    "    df_train.to_csv(\"pvc/train.csv\", index=False)\n",
    "    df_evaluation.to_csv(\"pvc/evaluation.csv\", index=False)\n",
    "\n",
    "    print(\"complete Loading Data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(create_component_from_func, base_image=\"679oose/basepython:1.0\")\n",
    "def train_model(\n",
    "    # train_path:InputPath(\"csv\"),\n",
    "    # evaluation_path: InputPath(\"csv\"),\n",
    "):\n",
    "\n",
    "    from transformers import (\n",
    "        DistilBertForSequenceClassification,\n",
    "        DistilBertTokenizer,\n",
    "        Trainer,\n",
    "        TrainingArguments,\n",
    "        TrainerCallback,\n",
    "    )\n",
    "\n",
    "    from datasets import Dataset\n",
    "\n",
    "    import os\n",
    "\n",
    "    print(\"list_dir : \\n \", os.listdir())\n",
    "    print(\"list_dir : \\n \", os.getcwd())\n",
    "    os.chdir(\"/\")\n",
    "\n",
    "    # loading data\n",
    "    # train_dataset = Dataset.from_csv(train_path).select(range(100))\n",
    "    # evaluation_dataset = Dataset.from_csv(evaluation_path)\n",
    "\n",
    "    train_dataset = Dataset.from_csv(\"pvc/train.csv\").select(range(32))\n",
    "    evaluation_dataset = Dataset.from_csv(\"pvc/evaluation.csv\")\n",
    "\n",
    "    # tokenizing\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "    def tokenize_function(item):\n",
    "        return tokenizer(item[\"text\"], padding=\"max_length\", max_length=128, truncation=True)\n",
    "\n",
    "    train = train_dataset.map(tokenize_function)\n",
    "    evaluation = evaluation_dataset.map(tokenize_function)\n",
    "\n",
    "    print(\"complete Tokenizing\")\n",
    "\n",
    "    model = DistilBertForSequenceClassification.from_pretrained(\n",
    "        \"distilbert-base-uncased\", num_labels=len(set(train_dataset[\"label\"]))\n",
    "    )\n",
    "\n",
    "    # BetterTransformer\n",
    "\n",
    "    tra_arg = TrainingArguments(\n",
    "        per_device_train_batch_size=8,\n",
    "        output_dir=\"test\",\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=2,\n",
    "        # evaluation_strategy=\"epoch\",\n",
    "        disable_tqdm=True,\n",
    "        save_strategy=\"no\",\n",
    "    )\n",
    "\n",
    "    class myCallback(TrainerCallback):\n",
    "        def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "            print(f\"{state.global_step} Steps \")\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=tra_arg,\n",
    "        train_dataset=train,\n",
    "        eval_dataset=evaluation,\n",
    "        callbacks=[myCallback],\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Saving Tokenizer, Model\n",
    "    trainer.save_model(\"pvc/torch_model\")\n",
    "    tokenizer.save_pretrained(\"pvc/torch_model\")\n",
    "\n",
    "    print(\"Saving Model & Tokenizer Complete !!\")\n",
    "\n",
    "    # config for torch\n",
    "    config = dict(\n",
    "        inference_address=\"http://0.0.0.0:8085\",\n",
    "        management_address=\"http://0.0.0.0:8085\",\n",
    "        metrics_address=\"http://0.0.0.0:8082\",\n",
    "        grpc_inference_port=7070,\n",
    "        grpc_management_port=7071,\n",
    "        enable_envvars_config=\"true\",\n",
    "        install_py_dep_per_model=\"true\",\n",
    "        model_store=\"model-store\",\n",
    "        model_snapshot={\n",
    "            \"name\": \"startup.cfg\",\n",
    "            \"modelCount\": 1,\n",
    "            \"models\": {\n",
    "                \"torch-model\": {  # Model Name\n",
    "                    \"1.0\": {\n",
    "                        \"defaultVersion\": \"true\",\n",
    "                        \"marName\": \"torch-model.mar\",\n",
    "                        \"minWorkers\": 1,\n",
    "                        \"maxWorkers\": 5,\n",
    "                        \"batchSize\": 1,\n",
    "                        \"maxBatchDelay\": 500,\n",
    "                        \"responseTimeout\": 120,\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "    \n",
    "    # making config & config folder\n",
    "    if not os.path.exists('pvc/torch_model/config') : \n",
    "        os.mkdir('pvc/torch_model/config')\n",
    "        \n",
    "    with open(\"pvc/torch_model/config/config.properties\", \"w\") as f:\n",
    "        for i, j in config.items():\n",
    "            f.write(f\"{i}={j}\\n\")\n",
    "        f.close()\n",
    "\n",
    "    print(\"Saving config.properties !!\")\n",
    "\n",
    "    # handler for torch\n",
    "    x = '''\n",
    "from abc import ABC\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class TransformersClassifierHandler(BaseHandler, ABC):\n",
    "    def __init__(self):\n",
    "        super(TransformersClassifierHandler, self).__init__()\n",
    "        self.initialized = False\n",
    "\n",
    "    def initialize(self, ctx):\n",
    "        self.manifest = ctx.manifest\n",
    "\n",
    "        properties = ctx.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\n",
    "            \"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "\n",
    "        self.model.to(self.device)\n",
    "        # BetterTransformer\n",
    "        self.model.eval()\n",
    "\n",
    "        logger.debug(f\"Transformer model from path {model_dir} loaded successfully\")\n",
    "\n",
    "        # Read the mapping file, index to object name\n",
    "        mapping_file_path = os.path.join(model_dir, \"index_to_name.json\")\n",
    "\n",
    "        if os.path.isfile(mapping_file_path):\n",
    "            with open(mapping_file_path) as f:\n",
    "                self.mapping = json.load(f)\n",
    "        else:\n",
    "            logger.warning(\n",
    "                \"Missing the index_to_name.json file. Inference output will not include class name.\"\n",
    "            )\n",
    "\n",
    "        self.initialized = True\n",
    "\n",
    "    def preprocess(self, data):\n",
    "        \"\"\"Very basic preprocessing code - only tokenizes.\n",
    "        Extend with your own preprocessing steps as needed.\n",
    "        \"\"\"\n",
    "        print(\"------- input data --------\")\n",
    "        print(data)\n",
    "        text = data[0].get(\"data\")\n",
    "        if text is None:\n",
    "            text = data[0].get(\"body\")\n",
    "\n",
    "        logger.info(f\"Received text: {text}\")\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(text, add_special_tokens=True, return_tensors=\"pt\")\n",
    "        return inputs\n",
    "\n",
    "    def inference(self, inputs):\n",
    "        \"\"\"\n",
    "        Predict the class of a text using a trained transformer model.\n",
    "        \"\"\"\n",
    "        # NOTE: This makes the assumption that your model expects text to be tokenized\n",
    "        # with \"input_ids\" and \"token_type_ids\" - which is true for some popular transformer models, e.g. bert.\n",
    "        # If your transformer model expects different tokenization, adapt this code to suit\n",
    "        # its expected input format.\n",
    "        inputs = inputs.to(self.device)\n",
    "\n",
    "        prediction = self.model(**inputs)[0].argmax().item()\n",
    "        logger.info(f\"Model predicted: {prediction}\")\n",
    "\n",
    "        if self.mapping:\n",
    "            prediction = self.mapping[str(prediction)]\n",
    "        return [prediction]\n",
    "\n",
    "    def postprocess(self, inference_output):\n",
    "        # TODO: Add any needed post-processing of the model predictions here\n",
    "        logger.info(f\"Model Name: {self.model.config._name_or_path}\")\n",
    "        logger.info(f\"Model predicted: {inference_output}\")\n",
    "        return inference_output\n",
    "\n",
    "\n",
    "_service = TransformersClassifierHandler()\n",
    "\n",
    "\n",
    "def handle(data, context):\n",
    "    try:\n",
    "        if not _service.initialized:\n",
    "            _service.initialize(context)\n",
    "\n",
    "        if data is None:\n",
    "            return None\n",
    "\n",
    "        data = _service.preprocess(data)\n",
    "        data = _service.inference(data)\n",
    "        data = _service.postprocess(data)\n",
    "\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        raise e\n",
    "\n",
    "\n",
    "    '''\n",
    "    with open(\"pvc/torch_model/handler.py\", \"w\") as f:\n",
    "        f.write(x)\n",
    "    f.close()\n",
    "\n",
    "    print(\"Saving handler.py complete !!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import ContainerOp\n",
    "\n",
    "\n",
    "def create_marfile():\n",
    "    return ContainerOp(\n",
    "        name=\"Creating Marfile\",\n",
    "        command=[\"/bin/sh\"],\n",
    "        image=\"python:3.9\",\n",
    "        arguments=[\n",
    "            \"-c\",\n",
    "            \"cd pvc/torch_model; pip install torchserve torch-model-archiver torch-workflow-archiver; torch-model-archiver --model-name torch-model --version 1.0 --serialized-file pytorch_model.bin --handler handler.py --extra-files config.json,vocab.txt --force; mkdir model-store; mv -f torch-model.mar model-store\"\n",
    "        ],  # pip install => create mar file => make model_store folder => mv marfile to model_store\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "@partial(\n",
    "    create_component_from_func,\n",
    "    packages_to_install=[\"kserve==0.8.0\"],\n",
    ")\n",
    "def create_inference_model():\n",
    "\n",
    "    from kserve import (\n",
    "        constants,\n",
    "        KServeClient,\n",
    "        V1beta1InferenceService,\n",
    "        V1beta1InferenceServiceSpec,\n",
    "        V1beta1PredictorSpec,\n",
    "        V1beta1ModelSpec,\n",
    "        V1beta1ModelFormat,\n",
    "        V1beta1TorchServeSpec,\n",
    "        utils,\n",
    "    )\n",
    "    from kubernetes import client\n",
    "\n",
    "    client.V1ResourceRequirements(requests={\"cpu\": 1, \"memory\": \"1G\"})\n",
    "\n",
    "    service_name = \"pytorchserve2\"\n",
    "    namespace = \"kubeflow-user-example-com\"\n",
    "    api_version = constants.KSERVE_GROUP + \"/\" + constants.KSERVE_V1BETA1_VERSION\n",
    "    storage = \"pvc://lee/torch_model\"\n",
    "\n",
    "    torchsvc = V1beta1InferenceService(\n",
    "        api_version=api_version,\n",
    "        kind=constants.KSERVE_KIND,\n",
    "        metadata=client.V1ObjectMeta(\n",
    "            name=service_name, namespace=namespace, annotations={\"sidecar.istio.io/inject\": \"false\"}\n",
    "        ),\n",
    "        spec=V1beta1InferenceServiceSpec(\n",
    "            predictor=V1beta1PredictorSpec(\n",
    "                pytorch=(\n",
    "                    V1beta1TorchServeSpec(\n",
    "                        protocol_version=\"v1\",\n",
    "                        resources=client.V1ResourceRequirements(\n",
    "                            requests={\"cpu\": 1, \"memory\": \"1G\"}\n",
    "                        ),\n",
    "                        storage_uri=storage,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "    )\n",
    "    KServe = KServeClient()\n",
    "    KServe.create(torchsvc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kfp.dsl import pipeline\n",
    "from kfp import onprem\n",
    "from kfp.dsl import ContainerOp\n",
    "\n",
    "@pipeline(name=\"NLP_Pipeline\")\n",
    "def NLP_Pipeline():\n",
    "    data = load_data()\n",
    "    data.apply(onprem.mount_pvc(pvc_name='lee', volume_name='test-lee', volume_mount_path=\"pvc\"))\n",
    "    # model = train_model(data.outputs['train'],data.outputs['evaluation'])    \n",
    "    model = train_model()\n",
    "    model.apply(onprem.mount_pvc(pvc_name='lee', volume_name='test-lee', volume_mount_path=\"pvc\"))\n",
    "    model.set_cpu_limit(cpu = '2').set_memory_limit(memory = '4G')\n",
    "    model.set_display_name('Finetuning Text Classification Model')\n",
    "    # model.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # 매번 새롭게 시작하게 하는 명령어\n",
    "    model.after(data)\n",
    "    \n",
    "    marfile=create_marfile()\n",
    "    marfile.apply(onprem.mount_pvc(pvc_name='lee', volume_name='test-lee', volume_mount_path=\"pvc\"))\n",
    "    marfile.set_display_name('Creating Marfile')\n",
    "    marfile.execution_options.caching_strategy.max_cache_staleness = \"P0D\" # 매번 새롭게 시작하게 하는 명령어\n",
    "    marfile.after(model)\n",
    "\n",
    "    inference_model=create_inference_model()\n",
    "    inference_model.apply(onprem.mount_pvc(pvc_name='lee', volume_name='test-lee', volume_mount_path=\"pvc\"))\n",
    "    inference_model.after(marfile)\n",
    "\n",
    "import kfp\n",
    "if __name__ == \"__main__\":\n",
    "    kfp.compiler.Compiler().compile(NLP_Pipeline, \"NLP_Pipeline.yaml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'apiVersion': 'serving.kserve.io/v1beta1',\n",
       " 'kind': 'InferenceService',\n",
       " 'metadata': {'annotations': {'sidecar.istio.io/inject': 'false'},\n",
       "  'creationTimestamp': '2023-01-21T07:07:18Z',\n",
       "  'generation': 1,\n",
       "  'labels': {'serviceEnvelope': 'kserve'},\n",
       "  'managedFields': [{'apiVersion': 'serving.kserve.io/v1beta1',\n",
       "    'fieldsType': 'FieldsV1',\n",
       "    'fieldsV1': {'f:metadata': {'f:annotations': {'.': {},\n",
       "       'f:sidecar.istio.io/inject': {}}},\n",
       "     'f:spec': {'.': {},\n",
       "      'f:predictor': {'.': {},\n",
       "       'f:pytorch': {'.': {},\n",
       "        'f:name': {},\n",
       "        'f:protocolVersion': {},\n",
       "        'f:resources': {'.': {},\n",
       "         'f:limits': {'.': {}, 'f:cpu': {}, 'f:memory': {}},\n",
       "         'f:requests': {'.': {}, 'f:cpu': {}, 'f:memory': {}}},\n",
       "        'f:storageUri': {}}}}},\n",
       "    'manager': 'OpenAPI-Generator',\n",
       "    'operation': 'Update',\n",
       "    'time': '2023-01-21T07:07:15Z'}],\n",
       "  'name': 'pytorchserve',\n",
       "  'namespace': 'kubeflow-user-example-com',\n",
       "  'resourceVersion': '144461',\n",
       "  'uid': 'a1699e07-c045-440b-8d03-7d7935a424f8'},\n",
       " 'spec': {'predictor': {'model': {'modelFormat': {'name': 'pytorch'},\n",
       "    'name': '',\n",
       "    'protocolVersion': 'v1',\n",
       "    'resources': {'limits': {'cpu': '1', 'memory': '2G'},\n",
       "     'requests': {'cpu': '500m', 'memory': '500M'}},\n",
       "    'runtime': 'kserve-torchserve',\n",
       "    'storageUri': 'pvc://lee/torch_model'}}}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kserve import (\n",
    "    constants,\n",
    "    KServeClient,\n",
    "    V1beta1InferenceService,\n",
    "    V1beta1InferenceServiceSpec,\n",
    "    V1beta1PredictorSpec,\n",
    "    V1beta1ModelSpec,\n",
    "    V1beta1ModelFormat,\n",
    "    V1beta1TorchServeSpec,\n",
    "    utils,\n",
    ")\n",
    "from kubernetes import client\n",
    "\n",
    "client.V1ResourceRequirements(requests={\"cpu\": 1, \"memory\": \"1G\"})\n",
    "\n",
    "service_name = \"pytorchserve\"\n",
    "namespace = \"kubeflow-user-example-com\"\n",
    "api_version = constants.KSERVE_GROUP + \"/\" + constants.KSERVE_V1BETA1_VERSION\n",
    "storage = \"pvc://lee/torch_model\"\n",
    "\n",
    "torchsvc = V1beta1InferenceService(\n",
    "    api_version=api_version,\n",
    "    kind=constants.KSERVE_KIND,\n",
    "    metadata=client.V1ObjectMeta(\n",
    "        name=service_name, namespace=namespace, annotations={\"sidecar.istio.io/inject\": \"false\"}\n",
    "    ),\n",
    "    spec=V1beta1InferenceServiceSpec(\n",
    "        predictor=V1beta1PredictorSpec(\n",
    "            pytorch=(\n",
    "                V1beta1TorchServeSpec(\n",
    "                    protocol_version=\"v1\",\n",
    "                    resources=client.V1ResourceRequirements(\n",
    "                        requests={\"cpu\": 0.5, \"memory\": \"0.5G\"},\n",
    "                        limits={\"cpu\": 1, \"memory\": \"2G\"}\n",
    "                    ),\n",
    "                    storage_uri=storage,\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "KServe = KServeClient()\n",
    "KServe.create(torchsvc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b2097164ba635ebffc0e3795dc845ae25b57eedf0c1eb5773ded6aee9fc1b279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
